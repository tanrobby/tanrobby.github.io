<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <title>YNC Machine Learning</title>
 
    <!-- Bootstrap core CSS -->
    <link href="../../bootstrap-3.3.6-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../jumbotron.css" rel="stylesheet">
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <div class="container"  style="font-size:16px">
        <h2>YSC3227 Machine Learning</h2>
        <p> Yale-NUS College
	</p>

      </div>
    </div>


    <div class="container" style="font-size:16px">
      
      
	<center>
      <p class="bg-info">
	<br>
      </p>
	</center>
      <br>

      <h3>Description:</h3>
      <p>
      The goal of machine learning is to enable machines  to  identify 
      patterns from data, extract the patterns, and based on
      them, make
      a prediction automatically. These capabilities are the core of
      artificial intelligence (namely, to make machines learn
      without being explicitly programmed using fixed predetermined rules). 
      The applications of machine learning are immense, since
      nowadays we are bombarded with a huge number of
      various data from various sources. We hope machine learning can make
      sense of this huge seemingly incomprehensible  data.
      </p>
      <br>
      <b>Prerequisite</b>: Programming skill in Python.
      <br>
      <b>Textbook</b>: <a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&qid=1461689550&sr=8-1&keywords=pattern+recognition+and+machine+learning"
      target="_blank">"Pattern Recognition and Machine Learning"</a>,
      by Christopher Bishop. 

      <br>
      <b>Instructor</b>: <a href="http://tanrobby.github.io">Robby
	T. Tan</a> (robby.tan [att] yale-nus.edu.sg)

      <br>
      <br>
      <hr>
      <h3 id="schedule">Schedule:</h3>
<p>
The following schedule will not be strictly followed.
</p>

<br>
      
<table width="100%" 
       border = "0"
       class="table table-striped">
  <thead class="thead-inverse">
    
    <tr>
      <th width="12%"> Date
      <th> <center> Topic </center>
      <th width="20%"> <center> Lecture Note </center>
  </thead>
  <tbody>
    
    
    <!------------------------------------------------------------------------------>	  
    
    
    <tr>
      <td rowspan="1"> August 15
      <td>
	
	<b>1. INTRODUCTION</b>
	<br>
	<br>
	Additional resources (optional):
	<ul>
	  <li> Introduction to Machine Learning:
	  <a href="http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf"
	  target="_blank">pdf</a> | <a href="https://www.youtube.com/watch?v=cKxRvEZd3Mw" target="_blank">youtube</a>
	</ul>
	<br>

      <td align="left">
	<a href="https://www.dropbox.com/s/g8n3kordszk830k/YSC4216_lecture01.pdf?dl=0"
	target="_blank">Lecture note 1</a> 
    </tr>
     <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> August 19
      <td>
	<b> 2. LEAST SQUARES REGRESSION (Part 1/2)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.1 (Polynomial Curve
	    Fitting)		    
	</ul>
	<td>
	<a href="https://www.dropbox.com/s/42314uve5ut2isn/YSC4216_lecture02.pdf?dl=0"
	target="_blank">Lecture note 2</a> 
    </tr>
	    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> August 22
      <td>
	<b> 3. LEAST SQUARES REGRESSION (Part 2/2)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Matrix Calculus: <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank">wikipedia</a>
	  <li>
	  Overfitting: <a href="https://en.wikipedia.org/wiki/Overfitting"
			  target="_blank">wikipedia</a>
	  <li> Overdetermined
	  systems: <a href="https://en.wikipedia.org/wiki/Overfitting"
		      target="_blank">wikipedia</a>
	  <li> Pseudoinverse: <a href="https://youtu.be/Go2aLo7ZOlU" target="_blank">online tutorial</a>	
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/7k6o52sr4ib8rz0/YSC4216_lecture03.pdf?dl=0" target="_blank">Lecture note 3</a>
	<br>
	<a href="assignment1.html" target="_blank">Assignment 1</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> August 26
      <td>
	<b>4. INTRODUCTION TO BAYESIAN INFERENCE</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.2 (Probability
	  Theory), Sect. 1.3 (Model Selection)
	  <li> Introduction to Bayesian
	  inference: <a href="http://videolectures.net/mlss09uk_bishop_ibi/?q=bayesian%20inference"
			target="_blank">video</a>
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/x0pbjuwcsuu6ejx/YSC4216_lecture04.pdf?dl=0" target="_blank">Lecture note 4</a>
    </tr>

    <!------------------------------------------------------------------------------>
    
	
    <tr>
      <td rowspan="1"> August 29
      <td>
	<b>5. MLE FOR REGRESSION</b>
	<br>
	<br>
	Reading:
	  <li> Chapter 1 (Introduction): Sect. 1.2.5 (Curve Fitting Re-Visited)
	<ul>
	  <li> Bayesian Inference: An Introduction to Principles and
	  Practice in Machine
	  Learning (Sect. 2.1, 2.1.1, and 2.1.2 only): <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
	Additional resources:
	<ul>
	  <li>
	  MLE: <a href="https://www.youtube.com/watch?v=aHwsEXCk4HA"
		  target="_blank">youtube</a>
 	  <li>
 	    MAP: <a href="https://www.youtube.com/watch?v=kkhdIriddSI" target="_blank">youtube</a>
	</ul>
	    <td align="left">
	      <a href="https://www.dropbox.com/s/pimmrmtm8fewsrv/YSC4216_lecture05.pdf?dl=0" target="_blank">Lecture note 5</a>
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> September 2
      <td>
	<b>6. MAP FOR REGRESSION</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 5
      <td>
	<b>6. BASIS FUNCTIONS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Linear Models for Regression (Sect. 3.1 only)
	</ul>
	Additional resources:
	<ul>
	  <li> Basis functions: <a href="https://youtu.be/rVviNyIR-fI"
	  target="_blank">youtube 1</a>
	  | <a href="https://youtu.be/wZk_uKEW_Oc" target="_blank">youtube 2</a>
	  <li> Spline
	  functions: <a href="http://geometrie.foretnik.net/files/NURBS-en.swf"
	  target="_blank">demo applet</a> (use Firefox)
	</ul>
      <td align="left">
	Assignment 2
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 9
      <td>
	<b>8. GAUSSIAN DISTRIBUTIONS: COVARIANCE MATRIX</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
	
   <tr>
     <td rowspan="1"> September 12
     <td> 
	<b>9. FULL BAYESIAN REGRESSION AND SEQUENTIAL LEARNING</b>
       <br>
       <br>
     <td align="left">
    </tr>
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> September 16
      <td>
       <b>10. PREDICTIVE DISTRIBUTION</b> 
	<br>
	<br>
      <td align="left">
    </tr>	
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  September 19
      <td>
	<b> 11. GAUSSIAN PROCESSES 1/2</b>
	<br>
	<br>
      <td align="left">
    </tr>
    <!------------------------------------------------------------------------------>

    <tr class="success">
      
      <td rowspan="1"> 
      <td>
	<b> RECESS WEEK </b>
	<br>
	<br>		
      <td>
    </tr>

    
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  September 30
      <td>
	<b>12.  GAUSSIAN PROCESSES 2/2</b> 
	<br>
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>

    <tr>
      
      <td rowspan="1"> October 3
      <td>
	<b>13. LEAST-SQUARES CLASSIFICATION (Part 1/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>


    <!------------------------------------------------------------------------------>


	
    <tr>
      <td rowspan="1"> October 7
      <td>
	<b>14. LEAST-SQUARES FOR CLASSIFICATION (Part 2/2)</b>
	<br>
	<br>	
    </tr>
    
    <!------------------------------------------------------------------------------>

	
    <tr>
      <td rowspan="1"> October 10
      <td>
	<b>15. MLE FOR CLASSIFICATION</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1"> October 14
      <td>
 	<b>16. MAP FOR CLASSIFICATION</b>
	<br>
	<br>
    </tr>
    
    <!------------------------------------------------------------------------------>

    
	
    <tr>
      <td rowspan="1"> October 17
      <td>
	<b>17. FULL BAYESIAN FOR CLASSIFICATION</b>
	<br>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> October 21
      <td>
	<b>18. PREDICTIVE DISTRIBUTION FOR CLASSIFICATION</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

    
    
    <tr>
      <td rowspan="1"> October 24
      <td>
	<b>19. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 1/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> October 31
      <td>
	<b>CLASS CANCELLED</b>
	<br>
	<br>
      <td align="left">
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> November 4
      <td>
	<b>20. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 2/2)</b>
	<br>
	<br>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>

	
    <tr>
	<td rowspan="1"> November 7
	<td>
	  <b>21. PROBABILISTIC GENERATIVE MODELS</b>
	  <br>
	  <br>
	<td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
	  
    
    <tr>
      <td rowspan="1"> November 11
      <td>
	  <b>22. SVM (SUPPORT VECTOR MACHINES)</b>
	  <br>
	  <br>
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> November 14
      <td>
	<b>23. MIXTURE MODELS AND EM</b>
	<br>
	<br>
	  <br>	
      <td align="left">
    </tr>
    
    <!------------------------------------------------------------------------------>

    
    <!------------------------------------------------------------------------------>
    

    <tr class="danger">
      <td rowspan="1">
      <td>
	<b>FINAL EXAM</b>
	<br>
	<br>
	<td align="left"> 
    </tr>


    
  </tbody>
</table>
<br>
      


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>

<br>
<br>
<br>
<hr>
<br>
<br>
<br>

      

</div>      
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="bootstrap-3.3.6-dist/js/bootstrap.min.js"></script>
  </body>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
<script type="text/javascript">
  try {
  var pageTracker = _gat._getTracker("UA-13131132-3");
  pageTracker._trackPageview();
  } catch(err) {}</script>
</html>
