<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <title>YNC Machine Learning</title>
 
    <!-- Bootstrap core CSS -->
    <link href="../../bootstrap-3.3.6-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../jumbotron.css" rel="stylesheet">
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <div class="container"  style="font-size:16px">
        <h2>YSC4216 Machine Learning</h2>
        <p> Yale-NUS College
	</p>
      </div>
    </div>

    <div class="container" style="font-size:16px">
      
      <h3>Description:</h3>
      <p>
      The goal of machine learning is to enable machines  to  identify 
      patterns from data, extract the patterns, and based on
      them, make
      a prediction automatically. These capabilities are the core of
      artificial intelligence (namely, to make machines learn
      without being explicitly programmed using fixed predetermined rules). 
      The applications of machine learning are immense, since
      nowadays we are bombarded with a huge number of
      various data from various sources. We hope machine learning can make
      sense of this huge seemingly incomprehensible  data.
      </p>
      <b>Textbook</b>: <a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&qid=1461689550&sr=8-1&keywords=pattern+recognition+and+machine+learning"
      target="_blank">"Pattern Recognition and Machine Learning"</a>,
      by Christopher Bishop. 

      <br>
      <b>Instructor</b>: <a href="http://tanrobby.github.io">Robby
	T. Tan</a> (robby.tan [att] yale-nus.edu.sg)

      <br>
      <hr>
      <h3 id="schedule">Schedule:</h3>
<p>
The following schedule is still tentative and not followed strictly.
</p>

<br>
      
<table width="100%" 
       border = "0"
       class="table table-striped">
  <thead class="thead-inverse">
    
    <tr>
      <th width="12%"> Date
      <th> <center> Topic </center>
      <th width="20%"> <center> Lecture Note </center>
  </thead>
  <tbody>
    
    
    <!------------------------------------------------------------------------------>	  
    
    
    <tr>
      <td rowspan="1"> January 10
      <td>
	
	<b>1. INTRODUCTION</b>
	<br>
	<br>
	Additional resources (optional):
	<ul>
	  <li> Introduction to Machine Learning:
	  <a href="http://www.cs.princeton.edu/courses/archive/spr08/cos511/scribe_notes/0204.pdf"
	  target="_blank">pdf</a> | <a href="https://www.youtube.com/watch?v=cKxRvEZd3Mw" target="_blank">youtube</a>
	</ul>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/3m1lg0pxhhkcok9/YSC4216_lecture01.pdf?dl=0" target="_blank">Lecture note 1</a>
    </tr>
     <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> January 13
      <td>
	<b> 2. LEAST SQUARES REGRESSION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.1 (Polynomial Curve
	    Fitting)
	  <li> Matrix Calculus: <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank">wikipedia</a>
	  <li>
	  Overfitting: <a href="https://en.wikipedia.org/wiki/Overfitting"
			  target="_blank">wikipedia</a>
	  <li> Overdetermined
	  systems: <a href="https://en.wikipedia.org/wiki/Overdetermined_system"
		      target="_blank">wikipedia</a>
	</ul>
	 Additional resources:
	<ul>
	  <li> Pseudoinverse: <a href="https://youtu.be/Go2aLo7ZOlU" target="_blank">online tutorial</a>	
	</ul>
      <td>
	<a href="https://www.dropbox.com/s/jh9qw0b9h5u5lnj/YSC4216_lecture02.pdf?dl=0" target="_blank">Lecture note 2</a>
    </tr>
	    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 17
      <td>
	<b> 3. BAYESIAN INFERENCE [1/2]</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1 (Introduction): Sect. 1.2 (Probability
	  Theory), Sect. 1.3 (Model Selection)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/xebfxnegkk6zl62/lecture03.pdf?dl=0"
	   target="_blank">Lecture note 3</a>
	<br>
	<a href="assignment1.html" target="_blank">Assignment 1</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 20
      <td>
	<b>4. BAYESIAN INFERENCE [2/2]</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Introduction to Bayesian
	  inference: <a href="http://videolectures.net/mlss09uk_bishop_ibi/?q=bayesian%20inference"
			target="_blank">video</a>
	  <li> Chapter 1 (Introduction): Sect. 1.2.5 (Curve Fitting Re-Visited)
	</ul>
      <td align="left">
    </tr>

    <!------------------------------------------------------------------------------>
    
	
    <tr>
      <td rowspan="1"> January 27
      <td>
	<b>5. MLE FOR REGRESSION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Bayesian Inference: An Introduction to Principles and
	  Practice in Machine
	  Learning (Sect. 2.1, 2.3, 2.4): <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
	Additional resources:
	<ul>
	  <li>
	  MLE: <a href="https://www.youtube.com/watch?v=aHwsEXCk4HA"
		  target="_blank">youtube</a>
	</ul>

      <td align="left">
	<a href="https://www.dropbox.com/s/x5h46nkesxxftpj/YSC4216_lecture05.pdf?dl=0" target="_blank">Lecture note 5</a>
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 31
      <td>
	<b>6. MAP FOR REGRESSION (Part 1/2)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Bayesian Inference: An Introduction to Principles and
	  Practice in Machine
	  Learning (Sect. 2.2, 2.5, 2.6.1): <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>

	Additional resources:
	<ul>
	  <li>
 	    MAP: <a href="https://www.youtube.com/watch?v=kkhdIriddSI" target="_blank">youtube</a>
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/fpmjidkdfh909zt/YSC4216_lecture06.pdf?dl=0" target="_blank">Lecture note 6</a>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> February 3
      <td>
	<b>7. MAP FOR REGRESSION (Part 2/2)</b>
	<br>
	<br>
      <td align="left">

    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> February 7
      <td>
	<b>8. BASIS FUNCTIONS</b>
	  <br>
	  <br>
	Reading:
	<ul>
	  <li> Chapter 3: Linear Models for Regression (Sect. 3.1 only)
	</ul>
	Additional resources:
	<ul>
	  <li> Basis functions: <a href="https://youtu.be/rVviNyIR-fI"
	  target="_blank">youtube 1</a>
	  | <a href="https://youtu.be/wZk_uKEW_Oc" target="_blank">youtube 2</a>
	  <li> Spline
	  functions: <a href="http://geometrie.foretnik.net/files/NURBS-en.swf"
	  target="_blank">demo applet</a> (use Firefox)
	</ul>

      <td align="left">
	<a href="https://www.dropbox.com/s/mtl4ol2i58suklq/YSC4216_lecture08.pdf?dl=0" target="_blank">Lecture note 8</a>
	<br>
	<a href="assignment2.html" target="_blank">Assignment 2</a>
    </tr>

    <!------------------------------------------------------------------------------>
	
   <tr>
     <td rowspan="1"> February 10
     <td> 
	<b>9. GAUSSIAN DISTRIBUTIONS: COVARIANCE MATRIX</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 2: Sect. 2.3.1 to 2.3.6 (Gaussian Distribution)
	  <li> Covariance
	    matrix: <a href="https://en.wikipedia.org/wiki/Covariance_matrix"
	    target="_blank">wikipedia</a>
	  <li> Geometric interpretation of covariance
	  matrix: <a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"
		     target="_blank">website</a>
	</ul>

     <td align="left">
       <a href="https://www.dropbox.com/s/7p9gokv3bxxznvs/YSC4216_lecture09.pdf?dl=0"
	  target="_blank">Lecture note 9</a>
       <br>
       <a href="https://www.dropbox.com/s/63vc4aoln9558fz/YSC4216_lecture09b.pdf?dl=0" target="_blank">Lecture note 9b</a>
   </tr>
    <!------------------------------------------------------------------------------>
    
    
    <tr>
      <td rowspan="1"> February 14
      <td>
	<b>10. FULL BAYESIAN REGRESSION</b>
       <br>
       <br>
	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.1 (Parameter Distribution)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/8sg2w8q0u8luoh1/YSC4216_lecture10.pdf?dl=0" target="_blank">Lecture note 10</a>
    </tr>	
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  February 17
      <td>
	<b>11. SEQUENTIAL LEARNING </b> 
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 3: Sec. 3.1.3 (Sequential Learning), and
	  Sec. 3.3.1 (Parameter Distribution)
	</ul>
      <td align="left">
	See Lecture note 10
    </tr>
    <!------------------------------------------------------------------------------>

    <tr class="success">
      
      <td rowspan="1"> 
      <td>
	<b> RECESS WEEK </b>
	<br>
	<br>		
      <td>
    </tr>

    
    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1">  February 28
      <td>
       <b>12. PREDICTIVE DISTRIBUTION</b> 
	<br>
	<br>
  	Reading:
	<ul>
	  <li> Chapter 3: Sect. 3.3.2 (Predictive Distribution)
	  <li> Bayesian Inference: An Introduction to Principles and
		    Practice in Machine
		    Learning (From Section 1 to Section 3 only): 
		    <a href="http://www.miketipping.com/papers/met-mlbayes.pdf"
		       target="_blank">pdf</a>
	</ul>
    <td align="left">
      <a href="https://www.dropbox.com/s/qxaswl425pjzdc5/YSC4216_lecture12.pdf?dl=0"
	 target="_blank">Lecture note 12</a>
      <br>
      <a href="assignment3.html" target="_blank">Assignment 3</a>
    </tr>
    
    
    <!------------------------------------------------------------------------------>

    <tr>
      
      <td rowspan="1"> March 3
      <td>
	<b>13. GAUSSIAN PROCESSES (Part 1/2)</b>
	<br>
	<br>
	      	Reading:
	<ul>
	  <li> Chapter 6: Sect. 6.4.1 (Linear Regression Revisited)
	  and Sect. 6.4.2 (Gaussian Processes for Regression)
	</ul>
	Additional resources:
	<ul>
	  <li> Gaussian
	  Processes: <a href="https://www.youtube.com/watch?v=vU6AiEYED9E"
	  target="_blank">youtube</a> 
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/mt97xbjx7buvtyb/YSC4216_lecture13.pdf?dl=0" target="_blank">Lecture note 13</a>
    </tr>


    <!------------------------------------------------------------------------------>


	
    <tr>
      <td rowspan="1"> March 7
      <td>
	<b>14. GAUSSIAN PROCESSES (Part 2/2)</b>
	<br>
	<br>
      <td align="left">
	See lecture note 13
    </tr>
    
    <!------------------------------------------------------------------------------>

	
    <tr>
      <td rowspan="1"> March 10
      <td>
	<b>15. CLASSIFICATION: INTRODUCTION </b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.1.1 (two classes), 4.1.2 (multiple classes).
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/lehsi61v1tjivop/YSC4216_lecture15.pdf?dl=0" target="_blank">Lecture note 15</a>
    </tr>

    <!------------------------------------------------------------------------------>

    
    <tr>
      <td rowspan="1"> March 14
      <td>
 	<b>16. LEAST SQUARES FOR CLASSIFICATION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.1.3 (least squares for classification)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/s7ke3nzb1llb3f4/YSC4216_lecture16.pdf?dl=0" target="_blank">Lecture note 16</a>
	<br>
	<a href="assignment4.html" target="_blank">Assignment 4</a>
    </tr>
    
    <!------------------------------------------------------------------------------>

    
	
    <tr>
      <td rowspan="1"> March 17
      <td>
	<b>17. MLE FOR CLASSIFICATION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Sect. 4.3.2 (logistic regression) and 4.3.3 (iterative reweighted least squares)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/vlxenauya71t9ul/YSC4216_lecture17.pdf?dl=0" target="_blank">Lecture note 17</a>
	<br>
    </tr>

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> March 21
      <td>
	<b>18. MAP FOR CLASSIFICATION</b>
	<br>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/75d7d8por587waa/YSC4216_lecture18.pdf?dl=0" target="_blank">Lecture note 18</a>
	<br>
    </tr>
    <!------------------------------------------------------------------------------>

    
    
    <tr>
      <td rowspan="1"> March 24
      <td>
	<b>19. FULL BAYESIAN FOR CLASSIFICATION</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.4 (The Laplace Approximation)
	  <li> Chapter 4: Sect. 4.5.1 (Bayesion Logistic Regression:
	  Laplace Approximation)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/cgkkkxbj7n820en/YSC4216_lecture19.pdf?dl=0" target="_blank">Lecture note 19</a>
    </tr>

    <!------------------------------------------------------------------------------>

        <tr>
      <td rowspan="1"> March 28
      <td>
	  <b>20. PREDICTIVE DISTRIBUTION FOR CLASSIFICATION</b>
	  <br>
	  <br>
	  	Reading:
	<ul>
	  <li> Chapter 4: Sect. 4.5.2 (Bayesion Logistic Regression:
	  Predictive Distribution)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/uxygj4si4yixkyc/YSC4216_lecture20.pdf?dl=0"
	target="_blank">Lecture note 20</a>
	<br>
	<a href="assignment5.html" target="_blank">Assignment 5</a>
	</tr>

    <!------------------------------------------------------------------------------>

    
    
    <tr>
      <td rowspan="1"> March 31
      <td>
	<b>21. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 1/2)</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 6: Sect. 6.4.5 (Gaussian Processes for
	  Classification) and 6.4.6 (Laplace Approximation)
	</ul>
      <td align="left">
	<a href="https://www.dropbox.com/s/93m63dw4y6ptivi/YSC4216_lecture21.pdf?dl=0" target="_blank">Lecture note 21</a>
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> April 4
      <td>
	<b>22. GAUSSIAN PROCESSES FOR CLASSIFICATION (Part 2/2)</b>
	  <br>
	  <br>

      <td align="left">
	See lecture note 21
    </tr>

    <!------------------------------------------------------------------------------>

	
    <tr>
	<td rowspan="1"> April 11
	<td>
	  <b>23. MULTICLASS LOGISTIC REGRESSION</b>
	  <br>
	  <br>
	  Reading:
	  <ul>
	    <li> Chapter 4: Sect. 4.3.4 (Multiclass logistic regression)
	  </ul>
	<td align="left">
	  <a href="https://www.dropbox.com/s/9o1e88dp5xweao7/YSC4216_lecture22.pdf?dl=0"
	  target="_blank"> Lecture note 23</a>
    </tr>

    <!------------------------------------------------------------------------------>
	  
    
    
    <!------------------------------------------------------------------------------>
    

    
    <!------------------------------------------------------------------------------>
    

    <tr class="danger">
      <td rowspan="1">
      <td>
	<b>FINAL EXAM</b>
	<br>
	<br>
	<td align="left"> 
    </tr>


    
  </tbody>
</table>
<br>
      


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>

<br>
<br>
<br>
<hr>
<br>
<br>
<br>

      

</html>
