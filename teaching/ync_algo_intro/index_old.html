<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- <link rel="icon" href="../../favicon.ico"> -->

    <title>YNC Advanced Algorithms</title>
 
    <!-- Bootstrap core CSS -->
    <link href="../../bootstrap-3.3.6-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../../jumbotron.css" rel="stylesheet">
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
    </nav>

    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div class="jumbotron">
      <div class="container"  style="font-size:16px">
        <h2>YSC2229 Data Structures and Algorithms</h2>
        <p> Yale-NUS College
	</p>

      </div>
    </div>


    <div class="container" style="font-size:16px">
      
      
	<center>
      <p class="bg-info">
	<br>
      </p>
	</center>
      <br>

      <h3>Description:</h3>
      <p>	
	The goal of algorithm design is to create an algorithms that
	can generate correct outputs in efficient running time.
	To measure the
	efficiency requires running time analyses, or rate of growth
	analysis, which is basically to assess whether the running
	time of an algorithms is either linear or quadratic or
	exponential with respect to the size of the input data.
	In this course, we will learn algorithm-and-data- structure
	basic design techniques and tools, as well as their runtime
	analysis. 
	Besides, we also study some algorithms that are widely
	used, such as, divide and conquer,
	dynamic programming, greedy algorithms, graph algorithms,
	linear programming, randomized algorithm, etc. We also discuss
	a few types of data structures, particularly dynamic data
	structures and their access time analysis: hashing, randomized
	binary search trees, and an access-time analysis called amortized
	analysis. This course will be exciting, since we will see many
	elegant and beautiful algorithms/analysis.
	
      </p>
      <br>
      <b>Textbook:</b> Cormen et
      al, <a href="http://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=sr_1_1?ie=UTF8&qid=1461689657&sr=8-1&keywords=introduction+to+algorithms"
      target="_blank">''Introduction to Algorithms''</a>, 
      third edition, MIT press  (its ebook version is accessible through NUS library) 
      <br>

      
      <b>Instructor</b>: <a href="http://tanrobby.github.io">Robby
      T. Tan</a> (robby.tan [att] yale-nus.edu.sg)
      <br>
      <br>



<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>


<br>
<hr>

<h3>Lecture Schedule:</h3>
<p>
The following schedule will not be strictly followed.
</p>
<br>

<table width="100%" 
       class="table table-striped">
  <thead class="thead-inverse">
    
    <tr>
      <th width="12%"> Date
      <th> <center> Topic </center>
      <th width="25%"> <center> Lecture Note </center>
  </thead>
  <tbody>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 14
      <td>
	<b>1. INTRODUCTION</b> 
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 1: The role of algorithms in computing
	  <li> Chapter 2: Getting started (Sec. 2.1 and 2.2)
	  <li> Chapter 3: Growth of Functions (Sec. 3.1 and  3.2)
	  <li> Appendix A.1: Summation formulas and properties
	</ul>
	<br>
	Additional Reading:
	<ul>
	    <li> "Discrete mathematics and its applications" (K.H. Rosen): Chapter
	      3.3 (Complexity of Algorithms), Chapter
	      3.2 (Growth of Functions)
	</ul>
	<br>
      <td align="left">    </tr>

    
    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> January 17
      <td>
	<b>2. ASYMPTOTIC NOTATIONS + RECURRENCE ALGORITHMS</b> 
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 2: Sec. 2.3.1: The divide-and-conquer approach
	  <li> Chapter 2: Sec. 2.3.2: Analyzing divide and conquer algorithms
	  <li> Chapter 4: Divide and Conquer (Introduction only)
	</ul>
	<br>
	Additional Reading (after the above sections are understood):
	<ul>
	  <li> Chapter 4: Sec. 4.1: The maximum-subarray problem
	  <li> Chapter 4: Sec. 4.2: Strassen's algorithm for matrix multiplication
	</ul>
	<br>
	<td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 21
	<td> 
	  <b>3. RECURRENCE ANALYSIS: SUBSTITUTION METHOD</b> 
	  <br>
	  <br>
	  Reading:
	  <ul>
	    <li> Chapter 4: Sec. 4.3: The subsitution method for
	      solving recurrences
	  </ul>
	  <br>
	Additional Reading:
	<ul>
	    <li> "Discrete mathematics and its applications"
	    (K.H. Rosen): Chapter 5: Sec. 5.1 (Mathematical
	    Induction), 5.3 (Recursive Definitions and Structural
	    Induction), 5.4 (Recursive Algorithms)
	      
	</ul>
	<br>
	<td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 24
      <td> 
	<b>4. RECURRENCE ANALYSIS: TREE + MASTER METHODS</b>
	  <br>
	  <br>
	Reading:
	<ul>
	<li> Chapter 4: Sec. 4.4: The recursion-tree method for
	  solving recurrences
	<li> Chapter 4: Sec. 4.5: The master method for solving
	recurrences 
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 28
      <td> 
	<b>5. RANDOMIZED ALGORITHMS: PROBABILISTIC ANALYSIS</b>
	<br>
	<br> 
	Reading:
	<ul>
	  <li> Chapter 5: Probabilistic Analysis and Randomized
	    Algorithms (Sec 5.1 to 5.4)
	  <li> Appendix C: C.1 (rules of sum and product), C.2
	    (probability), C.3 (discrete random variables) 
	</ul>
	<br>
      <td align="left">
    </tr>

    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> January 31
      <td>
	<b>6. RANDOMIZED QUICKSORT</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 7: QuickSort (Sec. 7.1 to 7.4) 
	  <li> Chapter 9 (median and order statistics): Sect. 9.2 (selection in expected linear time)
	</ul>
	<br>		
      <td align="left">
    </tr>
    

    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> February 7
      <td>
	<b>CLASS CANCELLED</b>
	<br>
	<br>
      <td align="left">
    </tr>
    

    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> February 11
      <td>
	<b>7. QUIZ 1</b>
	<br>
	<br>
      <td align="left">
	<a href="https://www.dropbox.com/s/7sd1fjw9z43u9d7/quiz1_samples.pdf?dl=0"
	target="_blank">Sample Quiz-1 Questions</a> 
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> February 14
      <td>
	<b>8. SORTING IN LINEAR TIME</b> 
	<br>
	<br>
	Reading:
	<ul> 
	  <li> Chapter 8 (Sorting in linear time): Sect. 8.1 (lower
	  bound for sorting), 8.2 (counting sort), and 8.3 (radix sort).
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> February 18
      <td>
	<b>9. UNIVERSAL HASHING: PART 1</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 11 (hash tables): sect. 11.1 (direct-address
	  table), sect. 11.2 (hash tables), sect. 11.3 (hash functions) 
	</ul>
	<br>
      <td align="left">
    </tr>
    <!------------------------------------------------------------------------------>


        <tr>
      <td rowspan="1"> February 19
      <td>
	<b>10: UNIVERSAL HASHING: PART 2</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 11 (hash tables): sect. 11.4 (Open Addressing)
	  and sect. 11.5. (Perfect Hashing) 
	</ul>
	<br>
      <td align="left">
	</tr>

	    <!------------------------------------------------------------------------------>


	    <tr>
      <td rowspan="1"> February 21
      <td>
	<b>11. AMORTIZED ANALYSIS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 17: Amortized algorithms: sect. 17.1 (aggregate
	  analysis), and sect. 17.2 (the accounting method).
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    

    
    <!------------------------------------------------------------------------------>
    <!------------------------------   RECESS WEEK  -------------------------------->
    <!------------------------------------------------------------------------------>
    
    <tr class="success">
      
      <td rowspan="1">
      <td>
	<br>	
	<b> RECESS WEEK </b>
	<br>
	<br>		
      <td>
    </tr>
    
    <!------------------------------------------------------------------------------>
    
    
    
    
    <tr>
      <td rowspan="1"> March 4
      <td>
	<b>12. AMORTIZED ANALYSIS: ACCOUNTING METHOD + DISCUSSION</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 7
      <td>
	<b>13. AMORTIZED ANALYSIS: POTENTIAL METHOD</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 17: Amortized algorithms: sect. 17.3 (the
	  potential method), and sect. 17.4 (dynamic tables)
	</ul>
	<br>
      <td align="left">
    </tr>

    
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 11
      <td>
	<b>14. DYNAMIC PROGRAMMING: PART 1</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 15 (dynamic programming): Sect. 15.1 (rod
	  cutting), Sect. 15.2 (matrix-chain multiplication),
	  Sect. 15.3 (elements of dynamic programming)
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 14
      <td>
	<b>15. DYNAMIC PROGRAMMING: PART 2</b>
	<br>
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 18
      <td>
	<b>16. GREEDY ALGORITHMS</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 16 (greedy algorithms): sect. 16.1 (an activity
	  selection problem), sect. 16.2 (elements of the greedy strategy) 
	</ul> 
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 21
      <td>
	<b>17. LINEAR PROGRAMMING</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 29 (linear programming): sect. 29.1 (standard
	  and slack forms), sect. 29.3 (the simplex algorithm)
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 25
      <td>
	<b>18. QUIZ 2</b>
	<br>
	<br>
      <td align="left">

    </tr>
    
    

    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> March 28
      <td>
	<b>19. SHORTEST PATHS AND LINEAR PROGRAMS</b>
	<br> 
	<br>
	Reading:
	<ul>
	  <li> Chapter 29 (linear programming): sect. 29.2
	    (formulating problems as linear programs)
	  <li> Chapter 26 (maximum flow): sect. 26.1 (flow networks),
	  sect. 26.2 (the Ford-Fulkerson method)
	  <li> Chapter 24 (single-source shortest paths): sect. 24.1
	    (the bellman-ford algorithm),
	    sect. 24.3 (dijkstra's algorithm)
	</ul>
	<br>
      <td align="left">
    </tr>
    

    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> April 1
      <td>
	<b>20.  NP-COMPLETENESS: PART I</b>
	<br> 
	<br>
	Reading:
	<ul>
	  <li> Chapter 34: NP Completeness: sect.34.1 (polynomial
	  time), sect. 34.2 (polynomial time verification), sect. 34.3
	  (NP-Completeness and reducibility).
	</ul>
	<br>	
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>
    
    
    
    <tr>
      <td rowspan="1"> April 4
      <td>
	<b>21.  NP-COMPLETENESS: PART II</b>	
	<br> 
	<br>
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> April 8
      <td> 
	<b>22. APPROXIMATION ALGORITHMS</b>	
	<br>
	<br>
	Reading:
	<ul>
	  <li> Chapter 35: Approximation Algorithms: Sect. 35.1 (the
	  vertex-cover problem), Sect. 35.4 (randomization and linear
	  programming), Sect. 35.2 (the traveling salesman problem)
	</ul>
	<br>	
      <td align="left">
    </tr>
    
    
    <!------------------------------------------------------------------------------>

    <tr>
      <td rowspan="1"> April 11
      <td>
	<b>23. RANDOMIZED ALGORITHMS: MARKOV CHAIN</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Markov Chain on 2-SAT and
	  3-SAT: <a href="http://people.seas.harvard.edu/~cs125/fall14/lec19.pdf"
	  target="_blank">pdf</a> 
	</ul>
	<br>
      <td align="left">
    </tr>
    
    
    
    <!------------------------------------------------------------------------------>
    
    <tr>
      <td rowspan="1"> April 15
      <td>
	<b>24. RANDOMIZED ALGORITHMS: MARKOV CHAIN MONTE CARLO</b>
	<br>
	<br>
	Reading:
	<ul>
	  <li> Simulated
	  annealing: <a href="https://en.wikipedia.org/wiki/Simulated_annealing"
			target="_blank">wikipedia</a>
	  <li> Traveling Salesman Problem using Simulated Annealing:
	    <a href="https://www.fourmilab.ch/documents/travelling/anneal/"
	    target="_blank">website 1</a>
	    | <a href="https://toddwschneider.com/posts/traveling-salesman-with-simulated-annealing-r-and-shiny/"
	    target="_blank">website 2</a>  
	</ul>
      <td align="left">
x    </tr>
    
    <!------------------------------------------------------------------------------>
    

    <tr class="danger">
      <td rowspan="1"> April 18
      <td>
	<b> 25. REVIEW</b>
	<br>
	<br>
	
      <td align="left"> 
    </tr>

    
    <!------------------------------------------------------------------------------>
    

    
  </tbody>
</table>
<br>
<br>
<br>


      

</div>      
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="bootstrap-3.3.6-dist/js/bootstrap.min.js"></script>
  </body>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
<script type="text/javascript">
  try {
  var pageTracker = _gat._getTracker("UA-13131132-3");
  pageTracker._trackPageview();
  } catch(err) {}</script>
</html>
